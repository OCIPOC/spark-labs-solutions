Lab 3.6 : Caching
========================================
#### Create dataset
```bash
$  cd ~/dev/spark-labs/data/text/twinkle
$  ./create-data-files.sh
# this will create a bunch of files
```

#### Start Spark Shell with enough memory
```bash
$   ~/apps/spark/bin/spark-shell   --executor-memory 4G
```

#### Following in Scala shell
```scala
// scala

val f = sc.textFile("../data/twinkle/500M.data")
f.count  // note the time took
f.count  // note the time took
f.count  // note the time took

f.cache   // cache in memory
// to cache in disk use
// f.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)
f.count  // note the time took
f.count  // note the time took

// --- loading data from amazon S3
val f = sc.textFile("s3n://elephantscale-public/data/text/twinkle/100M.data")
f.count
f.count
f.cache  // or  f.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)
f.count

```
