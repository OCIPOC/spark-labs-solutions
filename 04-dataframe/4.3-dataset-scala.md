# Solution for Dataset

```scala

// This is used to implicitly convert an RDD to a DataFrame.
import spark.implicits._
import org.apache.spark.sql._
import org.apache.spark.sql.types._

// == read DF using csv reader - no schema
val peopleDF = spark.read.
               option("header", "true").
               // option("inferSchema", "true").
               csv("../data/people/people2.csv")
peopleDF.columns
peopleDF.printSchema

// == read DF using csv reader with schema
val nameField = StructField("name", StringType)
val genderField = StructField("gender", StringType)
val ageField = StructField("age", IntegerType)
val weightField = StructField("weight", FloatType)
val peopleSchema = StructType(Array(nameField, genderField, ageField, weightField ))

val peopleDF2 = spark.read.option("header", "true").
         schema(peopleSchema).
         csv("../data/people/people2.csv")
peopleDF2.printSchema


// == reading RDD with custom schema and converting it to Dataset

final case class Person (
    name: String,
    gender: String,
    age:Integer,
    weight:Float
  )

val p = spark.sparkContext.textFile("../data/people/people.csv")
val peopleRDD = p.map (line => {
        val tokens = line.split(",")
        val name = tokens(0)
        val gender = tokens(1)
        val age = tokens(2).toInt
        val weight = tokens(3).toFloat
        new Person (name, gender, age, weight)
      })
peopleRDD.collect.foreach(println)

// convert to Dataset
val peopleDS = peopleRDD.toDS
// another approach
val peopleDS2 = spark.createDataset[Person](peopleRDD)
peopleDS.show

// access RDD from dataset
peopleDS.rdd

// perform operations in typed
peopleDS.filter(_.age > 20).show
    // +----+------+---+
    // |name|gender|age|
    // +----+------+---+
    // |John|     M| 35|
    // |Jane|     F| 40|
    // +----+------+---+



peopleDS.map( p => p.name.toUpperCase).show
    // +-----+
    // |value|
    // +-----+
    // | JOHN|
    // | JANE|
    // | MIKE|
    // |  SUE|
    // +-----+


// === convert Dataset to Dataframe
val df2 = peopleDS.toDF
// df2: org.apache.spark.sql.DataFrame = [name: string, gender: string ... 1 more field]

// === convert Dataframe to Dataset
val ds2 = df2.as[Person]
// ds2: org.apache.spark.sql.Dataset[Person] = [name: string, gender: string ... 1 more field]

// dataframe  & RDD
df2.rdd
// org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[51]

```
