{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching in SQL -- Part 2\n",
    "Understand Spark SQL caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.4G\n",
      "-rw-r--r-- 1 ubuntu ubuntu 338M Oct 30 06:46 clickstream-2015-01-01.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 338M Oct 30 06:47 clickstream-2015-01-02.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 338M Oct 30 06:48 clickstream-2015-01-03.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 338M Oct 30 06:48 clickstream-2015-01-04.json\n",
      "CPU times: user 702 Âµs, sys: 12.2 ms, total: 13 ms\n",
      "Wall time: 663 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "! [ ! -d /data/click-stream/json/ ] && cd /data/click-stream  && python gen-clickstream-json.py \n",
    "\n",
    "! ls -lh  /data/click-stream/json/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark...\n",
      "Spark found in :  /home/ubuntu/apps/spark\n",
      "Spark config:\n",
      "\t spark.app.name=TestApp\n",
      "\tspark.master=local[*]\n",
      "\texecutor.memory=2g\n",
      "\tspark.sql.warehouse.dir=/tmp/tmp2iiupuv6\n",
      "\tsome_property=some_value\n",
      "Spark UI running on port 4040\n"
     ]
    }
   ],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Read JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read JSON in 3,768.10 ms \n",
      "registered temp table clickstream\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='clickstream', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "clickstreamDF = spark.read.json(\"../data/click-stream/json\")\n",
    "t2 = time.perf_counter()\n",
    "print (\"Read JSON in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "clickstreamDF.createOrReplaceTempView(\"clickstream\")\n",
    "print (\"registered temp table clickstream\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+-----------------+------+----------+-------------+----------+\n",
      "| action|   campaign|cost|           domain|    ip|   session|    timestamp|      user|\n",
      "+-------+-----------+----+-----------------+------+----------+-------------+----------+\n",
      "|clicked|campaign_15| 175|          cnn.com|ip_983|session_15|1420156800000|user_52636|\n",
      "|blocked|campaign_17| 146|       google.com|ip_254|session_91|1420156800043|user_60446|\n",
      "|clicked|campaign_12|   0|   funnyordie.com|ip_410|session_11|1420156800086| user_1981|\n",
      "| viewed|campaign_15|   3|comedycentral.com|ip_296| session_2|1420156800129|user_95346|\n",
      "|blocked|campaign_15|  46|comedycentral.com|ip_409|session_70|1420156800172|user_79791|\n",
      "|blocked| campaign_4|  27|       flickr.com| ip_40|session_39|1420156800215|user_23461|\n",
      "|clicked| campaign_5|  11|       google.com|ip_902|session_17|1420156800258|user_58470|\n",
      "|clicked|campaign_18| 102|sf.craigslist.org|ip_212|session_49|1420156800301| user_5382|\n",
      "|blocked|campaign_15|  37|     usatoday.com|ip_452|session_64|1420156800344|user_86673|\n",
      "| viewed|campaign_18| 156|       google.com|ip_327|session_33|1420156800387|  user_125|\n",
      "+-------+-----------+----+-----------------+------+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## see table data\n",
    "spark.sql(\"select * from clickstream limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Query without caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|           domain| total|\n",
      "+-----------------+------+\n",
      "|sf.craigslist.org|402978|\n",
      "|       google.com|402750|\n",
      "|     facebook.com|402632|\n",
      "|        bbc.co.uk|402438|\n",
      "|      foxnews.com|402305|\n",
      "|       amazon.com|402290|\n",
      "|      youtube.com|402262|\n",
      "|       flickr.com|402061|\n",
      "|     usatoday.com|402054|\n",
      "|         hulu.com|401965|\n",
      "+-----------------+------+\n",
      "\n",
      "query took 2,258.81 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select domain, count(*) as total from clickstream\n",
    "group by domain \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_domains = spark.sql(sql)\n",
    "top10_domains.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Explain Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'GlobalLimit 10\n",
      "+- 'LocalLimit 10\n",
      "   +- 'Sort ['total DESC NULLS LAST], true\n",
      "      +- 'Aggregate ['domain], ['domain, 'count(1) AS total#88]\n",
      "         +- 'UnresolvedRelation [clickstream]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "domain: string, total: bigint\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [total#88L DESC NULLS LAST], true\n",
      "      +- Aggregate [domain#10], [domain#10, count(1) AS total#88L]\n",
      "         +- SubqueryAlias clickstream\n",
      "            +- Relation[action#7,campaign#8,cost#9L,domain#10,ip#11,session#12,timestamp#13L,user#14] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [total#88L DESC NULLS LAST], true\n",
      "      +- Aggregate [domain#10], [domain#10, count(1) AS total#88L]\n",
      "         +- Project [domain#10]\n",
      "            +- Relation[action#7,campaign#8,cost#9L,domain#10,ip#11,session#12,timestamp#13L,user#14] json\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[total#88L DESC NULLS LAST], output=[domain#10,total#88L])\n",
      "+- *(2) HashAggregate(keys=[domain#10], functions=[count(1)], output=[domain#10, total#88L])\n",
      "   +- Exchange hashpartitioning(domain#10, 200), true, [id=#97]\n",
      "      +- *(1) HashAggregate(keys=[domain#10], functions=[partial_count(1)], output=[domain#10, count#100L])\n",
      "         +- FileScan json [domain#10] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/data/click-stream/json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<domain:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#top10_domains.explain()\n",
    "\n",
    "top10_domains.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Cache\n",
    "\n",
    "There are 3 ways to cache\n",
    "1. dataframe.cache()  : non blocking\n",
    "2. spark.sql(\"cache table TABLE_NAME\") : blocking\n",
    "3. spark.catalog.cacheTable('tableName') : non blocking\n",
    "\n",
    "Try all these options and see the performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 'clickstream' cached :  False\n",
      "caching took 13.89 ms \n",
      "is 'clickstream' cached :  True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# uncache\n",
    "spark.catalog.clearCache() ## clear all tables\n",
    "# spark.catalog.uncacheTable(\"clickstream\")  # clear just one table\n",
    "\n",
    "print (\"is 'clickstream' cached : \" , spark.catalog.isCached('clickstream'))\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "## we have different ways to cache,\n",
    "## uncomment one of the following\n",
    "#spark.sql(\"cache table clickstream\");  ## 1\n",
    "#clickstreamDF.cache()  ## 2\n",
    "spark.catalog.cacheTable(\"clickstream\") ## 3\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"caching took {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "print (\"is 'clickstream' cached : \" , spark.catalog.isCached('clickstream'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step : Query after caching\n",
    "Run the following cell to measure query time after caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|           domain| total|\n",
      "+-----------------+------+\n",
      "|sf.craigslist.org|402978|\n",
      "|       google.com|402750|\n",
      "|     facebook.com|402632|\n",
      "|        bbc.co.uk|402438|\n",
      "|      foxnews.com|402305|\n",
      "|       amazon.com|402290|\n",
      "|      youtube.com|402262|\n",
      "|       flickr.com|402061|\n",
      "|     usatoday.com|402054|\n",
      "|         hulu.com|401965|\n",
      "+-----------------+------+\n",
      "\n",
      "query took 8,105.17 ms \n"
     ]
    }
   ],
   "source": [
    "## Query1 after caching\n",
    "## Note the time taken\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select domain, count(*) as total from clickstream\n",
    "group by domain \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_domains = spark.sql(sql)\n",
    "top10_domains.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|           domain| total|\n",
      "+-----------------+------+\n",
      "|sf.craigslist.org|402978|\n",
      "|       google.com|402750|\n",
      "|     facebook.com|402632|\n",
      "|        bbc.co.uk|402438|\n",
      "|      foxnews.com|402305|\n",
      "|       amazon.com|402290|\n",
      "|      youtube.com|402262|\n",
      "|       flickr.com|402061|\n",
      "|     usatoday.com|402054|\n",
      "|         hulu.com|401965|\n",
      "+-----------------+------+\n",
      "\n",
      "query took 332.10 ms \n"
     ]
    }
   ],
   "source": [
    "## Note the time for second query\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select domain, count(*) as total from clickstream\n",
    "group by domain \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_domains = spark.sql(sql)\n",
    "top10_domains.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step : Explain Query\n",
    "You will see caching in effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[total#313L DESC NULLS LAST], output=[domain#10,total#313L])\n",
      "+- *(2) HashAggregate(keys=[domain#10], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(domain#10, 200), true, [id=#200]\n",
      "      +- *(1) HashAggregate(keys=[domain#10], functions=[partial_count(1)])\n",
      "         +- Scan In-memory table clickstream [domain#10]\n",
      "               +- InMemoryRelation [action#7, campaign#8, cost#9L, domain#10, ip#11, session#12, timestamp#13L, user#14], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- FileScan json [action#7,campaign#8,cost#9L,domain#10,ip#11,session#12,timestamp#13L,user#14] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/data/click-stream/json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<action:string,campaign:string,cost:bigint,domain:string,ip:string,session:string,timestamp...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top10_domains.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Cache\n",
    "Try the following ways to clear cache\n",
    "\n",
    "1. spark.sql (\"CLEAR CACHE\")  - removes all cache\n",
    "2. spark.sql (\"CLEAR CACHE tableName\"); - removes one table\n",
    "3. spark.catalog.uncacheTable('tableName') - removes one cached table\n",
    "4. spark.catalog.clearCache() - clear all caches\n",
    "5. dataframe.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"CLEAR CACHE\")\n",
    "#spark.sql(\"CLEAR CACHE clickstream\");\n",
    "spark.catalog.uncacheTable('clickstream')\n",
    "#spark.catalog.clearCache() \n",
    "#top10_domains.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
