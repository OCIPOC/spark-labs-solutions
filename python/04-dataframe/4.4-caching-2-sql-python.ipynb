{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching in SQL -- Part 2\n",
    "Understand Spark SQL caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark...\n",
      "Spark found in :  /home/ubuntu/spark\n",
      "Spark config:\n",
      "\t spark.app.name=TestApp\n",
      "\tspark.master=local[*]\n",
      "\texecutor.memory=2g\n",
      "\tspark.sql.warehouse.dir=/tmp/tmpdcgl02or\n",
      "\tsome_property=some_value\n",
      "Spark UI running on port 4044\n"
     ]
    }
   ],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Read JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read JSON in 13,921.91 ms \n",
      "registered temp table clickstream\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='clickstream', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "clickstreamDF = spark.read.json(\"/data/click-stream/json\")\n",
    "t2 = time.perf_counter()\n",
    "print (\"Read JSON in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "clickstreamDF.createOrReplaceTempView(\"clickstream\")\n",
    "print (\"registered temp table clickstream\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+-----------------+------+----------+-------------+----------+\n",
      "| action|   campaign|cost|           domain|    ip|   session|    timestamp|      user|\n",
      "+-------+-----------+----+-----------------+------+----------+-------------+----------+\n",
      "|blocked|campaign_12| 108|   funnyordie.com|ip_301|session_95|1420329600000|user_83559|\n",
      "| viewed| campaign_5|  74|         hulu.com|ip_791|session_62|1420329600043|user_89610|\n",
      "|blocked| campaign_4| 127|sf.craigslist.org|ip_998|session_90|1420329600086|user_82694|\n",
      "|clicked|campaign_14|  57|        bbc.co.uk|ip_514|session_84|1420329600129|user_43516|\n",
      "|blocked|campaign_18|   2|   funnyordie.com|ip_799|session_46|1420329600172|user_85431|\n",
      "| viewed|campaign_12|   7|sf.craigslist.org|ip_101|session_89|1420329600215|user_48910|\n",
      "| viewed|campaign_12|  79|          npr.org|ip_535|session_49|1420329600258|user_43425|\n",
      "| viewed| campaign_6|  14|        yahoo.com| ip_26|session_45|1420329600301|user_52671|\n",
      "|blocked|campaign_19| 143|comedycentral.com|ip_355|session_41|1420329600344|user_32334|\n",
      "|blocked| campaign_6|  35|     facebook.com|ip_970|session_26|1420329600387|user_14329|\n",
      "+-------+-----------+----+-----------------+------+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## see table data\n",
    "spark.sql(\"select * from clickstream limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Query without caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|           domain| total|\n",
      "+-----------------+------+\n",
      "|      youtube.com|402597|\n",
      "|      nytimes.com|402569|\n",
      "|          npr.org|402521|\n",
      "|sf.craigslist.org|402374|\n",
      "|        zynga.com|402372|\n",
      "|       sfgate.com|402333|\n",
      "|     facebook.com|402101|\n",
      "|       google.com|402044|\n",
      "|comedycentral.com|402003|\n",
      "|       flickr.com|401926|\n",
      "+-----------------+------+\n",
      "\n",
      "query took 8,812.81 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select domain, count(*) as total from clickstream\n",
    "group by domain \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_domains = spark.sql(sql)\n",
    "top10_domains.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Explain Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'GlobalLimit 10\n",
      "+- 'LocalLimit 10\n",
      "   +- 'Sort ['total DESC NULLS LAST], true\n",
      "      +- 'Aggregate ['domain], ['domain, 'count(1) AS total#74]\n",
      "         +- 'UnresolvedRelation `clickstream`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "domain: string, total: bigint\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [total#74L DESC NULLS LAST], true\n",
      "      +- Aggregate [domain#9], [domain#9, count(1) AS total#74L]\n",
      "         +- SubqueryAlias `clickstream`\n",
      "            +- Relation[action#6,campaign#7,cost#8L,domain#9,ip#10,session#11,timestamp#12L,user#13] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [total#74L DESC NULLS LAST], true\n",
      "      +- Aggregate [domain#9], [domain#9, count(1) AS total#74L]\n",
      "         +- Project [domain#9]\n",
      "            +- Relation[action#6,campaign#7,cost#8L,domain#9,ip#10,session#11,timestamp#12L,user#13] json\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[total#74L DESC NULLS LAST], output=[domain#9,total#74L])\n",
      "+- *(2) HashAggregate(keys=[domain#9], functions=[count(1)], output=[domain#9, total#74L])\n",
      "   +- Exchange hashpartitioning(domain#9, 200)\n",
      "      +- *(1) HashAggregate(keys=[domain#9], functions=[partial_count(1)], output=[domain#9, count#86L])\n",
      "         +- *(1) FileScan json [domain#9] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/data/click-stream/json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<domain:string>\n"
     ]
    }
   ],
   "source": [
    "#top10_domains.explain()\n",
    "\n",
    "top10_domains.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Cache\n",
    "\n",
    "There are 3 ways to cache\n",
    "1. dataframe.cache()  : non blocking\n",
    "2. spark.sql(\"cache table TABLE_NAME\") : blocking\n",
    "3. spark.catalog.cacheTable('tableName') : non blocking\n",
    "\n",
    "Try all these options and see the performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 'clickstream' cached :  False\n",
      "caching took 20.28 ms \n",
      "is 'clickstream' cached :  True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# uncache\n",
    "spark.catalog.clearCache() ## clear all tables\n",
    "# spark.catalog.uncacheTable(\"clickstream\")  # clear just one table\n",
    "\n",
    "print (\"is 'clickstream' cached : \" , spark.catalog.isCached('clickstream'))\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "## we have different ways to cache,\n",
    "## uncomment one of the following\n",
    "#spark.sql(\"cache table clickstream\");  ## 1\n",
    "#clickstreamDF.cache()  ## 2\n",
    "spark.catalog.cacheTable(\"clickstream\") ## 3\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"caching took {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "print (\"is 'clickstream' cached : \" , spark.catalog.isCached('clickstream'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step : Query after caching\n",
    "Run the following cell to measure query time after caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|           domain| total|\n",
      "+-----------------+------+\n",
      "|      youtube.com|402597|\n",
      "|      nytimes.com|402569|\n",
      "|          npr.org|402521|\n",
      "|sf.craigslist.org|402374|\n",
      "|        zynga.com|402372|\n",
      "|       sfgate.com|402333|\n",
      "|     facebook.com|402101|\n",
      "|       google.com|402044|\n",
      "|comedycentral.com|402003|\n",
      "|       flickr.com|401926|\n",
      "+-----------------+------+\n",
      "\n",
      "query took 29,379.45 ms \n"
     ]
    }
   ],
   "source": [
    "## Query1 after caching\n",
    "## Note the time taken\n",
    "\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select domain, count(*) as total from clickstream\n",
    "group by domain \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_domains = spark.sql(sql)\n",
    "top10_domains.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|           domain| total|\n",
      "+-----------------+------+\n",
      "|      youtube.com|402597|\n",
      "|      nytimes.com|402569|\n",
      "|          npr.org|402521|\n",
      "|sf.craigslist.org|402374|\n",
      "|        zynga.com|402372|\n",
      "|       sfgate.com|402333|\n",
      "|     facebook.com|402101|\n",
      "|       google.com|402044|\n",
      "|comedycentral.com|402003|\n",
      "|       flickr.com|401926|\n",
      "+-----------------+------+\n",
      "\n",
      "query took 1,524.31 ms \n"
     ]
    }
   ],
   "source": [
    "## Note the time for second query\n",
    "import time\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "sql=\"\"\"\n",
    "select domain, count(*) as total from clickstream\n",
    "group by domain \n",
    "order by total desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "top10_domains = spark.sql(sql)\n",
    "top10_domains.show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"query took {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step : Explain Query\n",
    "You will see caching in effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[total#215L DESC NULLS LAST], output=[domain#9,total#215L])\n",
      "+- *(2) HashAggregate(keys=[domain#9], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(domain#9, 200)\n",
      "      +- *(1) HashAggregate(keys=[domain#9], functions=[partial_count(1)])\n",
      "         +- InMemoryTableScan [domain#9]\n",
      "               +- InMemoryRelation [action#6, campaign#7, cost#8L, domain#9, ip#10, session#11, timestamp#12L, user#13], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) FileScan json [action#6,campaign#7,cost#8L,domain#9,ip#10,session#11,timestamp#12L,user#13] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/data/click-stream/json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<action:string,campaign:string,cost:bigint,domain:string,ip:string,session:string,timestamp...\n"
     ]
    }
   ],
   "source": [
    "top10_domains.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Cache\n",
    "Try the following ways to clear cache\n",
    "\n",
    "1. spark.sql (\"CLEAR CACHE\")  - removes all cache\n",
    "2. spark.sql (\"CLEAR CACHE tableName\"); - removes one table\n",
    "3. spark.catalog.uncacheTable('tableName') - removes one cached table\n",
    "4. spark.catalog.clearCache() - clear all caches\n",
    "5. dataframe.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"CLEAR CACHE\")\n",
    "#spark.sql(\"CLEAR CACHE clickstream\");\n",
    "spark.catalog.uncacheTable('clickstream')\n",
    "#spark.catalog.clearCache() \n",
    "#top10_domains.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
