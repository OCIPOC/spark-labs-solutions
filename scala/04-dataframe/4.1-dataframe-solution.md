Lab 4.1 : Dataframe solution
============================


#### Start Spark Shell
```bash
$   ~/apps/spark/bin/spark-shell
```

#### Following in Scala shell
```scala

  val clickstreamDF = spark.read.json("/data/click-stream/clickstream.json")

  // output :
  // clickstreamDF: org.apache.spark.sql.DataFrame = [action: string, campaign: string, cost: bigint, domain: string, ip: string, session: string, timestamp: bigint, user: string]

  // schema
  clickstreamDF.printSchema
  /* output
  root
   |-- action: string (nullable = true)
   |-- campaign: string (nullable = true)
   |-- cost: long (nullable = true)
   |-- domain: string (nullable = true)
   |-- ip: string (nullable = true)
   |-- session: string (nullable = true)
   |-- timestamp: long (nullable = true)
   |-- user: string (nullable = true)
   */

  clickstreamDF.show
  // nicely formated table output

  import spark.implicits._

  // Show only click logs where the cost > 100
  clickstreamDF.filter(clickstreamDF("cost") > 100).show()
  clickstreamDF.filter("cost > 100").show()
  clickstreamDF.filter($"cost" > 100).show()

  // Show the logs where action = clicked
  clickstreamDF.filter(clickstreamDF("action") === "clicked").show
  // another way
  clickstreamDF.filter(clickstreamDF("action").equalTo("clicked")).show

  // show traffic per domain
  // step by step
  val g = clickstreamDF.groupBy("domain")
  val c = g.count
  c.show

  // one-liner
  clickstreamDF.groupBy("domain").count.show


  // load domain info
  val domainsDF = spark.read.json("/data/click-stream/domain-info.json")
  domainsDF.show

  // inner join
  val joined = clickstreamDF.join(domainsDF,  clickstreamDF("domain") === domainsDF("domain"))
  joined.show

  // outer join
  val joinedOuter = clickstreamDF.join(domainsDF,  clickstreamDF("domain") === domainsDF("domain"), "outer")
  joinedOuter.show

```
